{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection\n",
    "import time\n",
    "import pickle\n",
    "#import antigravity\n",
    "import random\n",
    "random.seed()\n",
    "import sklearn.neural_network\n",
    "from collections import namedtuple\n",
    "from handydict import dict as hd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 91)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_path = 'data\\\\train.csv'\n",
    "data_test_path = 'data\\\\test.csv'\n",
    "src_data_train = pd.read_csv(data_train_path)\n",
    "src_data_test = pd.read_csv(data_test_path)\n",
    "src_data_test['SalePrice']=np.NaN # training target is denoted by NaN\n",
    "source_data = pd.concat((src_data_train,src_data_test))\n",
    "source_data=source_data.set_index('Id')\n",
    "rows_test=np.where(np.isnan(source_data['SalePrice']))[0] # these are used for my own training and validation of the model, and must suffice for all validation actions\n",
    "rows_train=np.where(~np.isnan(source_data['SalePrice']))[0] # Note: These have no target label. These are only used for final submission\n",
    "\n",
    "# get the target transformer so that I can un-transform the target for final error evaluation\n",
    "p_target_basic = pd.DataFrame(source_data['SalePrice'])\n",
    "with open('target_transformer.pickle','rb') as f:\n",
    "    pr_target_pipe = pickle.load(f)\n",
    "\n",
    "# get the cleaned, augumented data\n",
    "p_data = pd.read_pickle('p_data.pickle')\n",
    "pr_target_log = pd.read_pickle('p_target_transformed.pickle')\n",
    "p_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additionally rescale the data - NNs are supposedely sensitive to data scaling\n",
    "p_data_scaler = sklearn.preprocessing.RobustScaler()\n",
    "training_data = p_data_scaler.fit_transform(p_data)[rows_train,:]\n",
    "\n",
    "# training_target is already transformed\n",
    "training_target = pr_target_log.iloc[rows_train].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_function(nn_width,nn_depth):\n",
    "    '''\n",
    "    create a tuple of sizes for layers out of two parameters: initial layer size, and count of layers\n",
    "    '''\n",
    "    return tuple(np.int32(np.logspace(np.log10(nn_width),np.log10(5.0),nn_depth)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(119, 54, 24, 11, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers_function(120,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Concept: grid search for grid_size, grid_nn_depth, alpha, learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_errors=[]\n",
    "t_learning_costs=[]\n",
    "worst_error_for_this_config=999999\n",
    "best_config_validation_error=np.NaN\n",
    "this_config_errors=[]\n",
    "configs=[]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha (L2 regularizer) rates: [1.e-06 1.e-05 1.e-04 1.e-03]\n",
      "learning rates: [1.e-05 1.e-04 1.e-03 1.e-02 1.e-01]\n"
     ]
    }
   ],
   "source": [
    "grid_alphas = np.logspace(-6,-3,4); print(f'alpha (L2 regularizer) rates: {grid_alphas}');\n",
    "grid_nn_widths=[25,50,75,125,200]\n",
    "grid_nn_depths=[2,4,6,8,10]\n",
    "grid_learningrates=np.logspace(-5,-1,5); print(f'learning rates: {grid_learningrates}');\n",
    "grid_activations=['logistic','tanh','relu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs=[]\n",
    "for idxa, learningrate in enumerate(grid_learningrates):\n",
    "    for idxb, alpha in enumerate(grid_alphas):\n",
    "        for idxc, nn_width in enumerate(grid_nn_widths):\n",
    "            for idxd, nn_depth in enumerate(grid_nn_depths):            \n",
    "                for idxe, activation in enumerate(grid_activations):\n",
    "                    this_config=hd({'idx':(idxa,idxb,idxc,idxd,idxe), 'alpha':alpha, 'learningrate':learningrate, 'nn_width':nn_width,'nn_depth':nn_depth,'activation':activation})\n",
    "                    configs.append(this_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82c401d58ab04174b98cb416dcc2d06e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1500.0), HTML(value='')), layout=Layout(dâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "best_alpha=-1\n",
    "best_nn_width=-1\n",
    "best_depth=-1\n",
    "best_learningrate=-1\n",
    "best_activation=''\n",
    "best_validation_error=9999999\n",
    "\n",
    "with tqdm(configs,ncols='100%') as pbar:\n",
    "    pbar.set_description(f'e: {best_validation_error:0.1f} for alpha:{best_alpha}, lr:{best_learningrate}, width:{best_nn_width}, depth:{best_depth}, {best_activation}')\n",
    "    for config in pbar:\n",
    "        # prepare as per config\n",
    "        basic_nn = sklearn.neural_network.MLPRegressor(\n",
    "            hidden_layer_sizes=layers_function(config.nn_width,config.nn_depth),\n",
    "            activation=config.activation,  \n",
    "            alpha=config.alpha,\n",
    "            learning_rate_init=config.learningrate,\n",
    "            random_state=None,\n",
    "            verbose=False,\n",
    "            early_stopping=True,\n",
    "            n_iter_no_change =20,\n",
    "            tol = (pr_target_pipe.transform([[180050]])-pr_target_pipe.transform([[180000]]))[0][0], # tolerance of 50 USD on the price of the home\n",
    "            validation_fraction=0.03,\n",
    "            max_iter=20000)\n",
    "        # execute\n",
    "        # OK, I do not believe the SKLearn's internal validator all that much. I'd rather split the dataset by hand anyway.\n",
    "        t0=time.perf_counter_ns()\n",
    "        rs = sklearn.model_selection.ShuffleSplit(n_splits=3, test_size=0.03, random_state=random.randint(1,1024))   \n",
    "        local_validation_errors=[]\n",
    "        for train_index, validation_index in rs.split(rows_train):\n",
    "            basic_nn.fit(training_data[train_index,:],training_target[train_index])            \n",
    "            # evaluate \n",
    "            # note: since this regressor is self-validating AND early-stopping, check final error on entire dataset.\n",
    "            validation_prediction=basic_nn.predict(training_data[validation_index,:]).reshape(1, -1)\n",
    "            validation_target_values = training_target[validation_index].reshape(1, -1)\n",
    "            local_validation_error=np.sqrt(np.mean(np.square(pr_target_pipe.inverse_transform(validation_target_values)-pr_target_pipe.inverse_transform(validation_prediction))))\n",
    "            local_validation_errors.append(local_validation_error)\n",
    "        validation_error=np.max(local_validation_errors)  # only remember the worst validation error. I do not want an overfitter!\n",
    "        t_learning_cost=time.perf_counter_ns()-t0\n",
    "        t_learning_costs.append(t_learning_cost)\n",
    "        validation_errors.append(validation_error)\n",
    "        if validation_error<best_validation_error:\n",
    "            best_validation_error=validation_error\n",
    "            best_alpha=config.alpha\n",
    "            best_learningrate = config.learningrate\n",
    "            best_depth=config.nn_depth\n",
    "            best_nn_width=config.nn_width\n",
    "            best_activation=config.activation\n",
    "            pbar.set_description(f'e: {best_validation_error:0.1f} for alpha:{best_alpha}, lr:{best_learningrate}, width:{best_nn_width}, depth:{best_depth}, {best_activation}')          \n",
    "            # save the state of the nn, can be loaded later from file\n",
    "            with open('best_nn.pickle','wb') as file_handle: # save the pr_target_pipe state\n",
    "                pickle.dump(basic_nn,file_handle)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
